{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy import interp\n",
    "import pandas.core.algorithms as algos \n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/data1/pypackages_cris ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "def my_custom_loss_func(y_true,y_pred):\n",
    "    \n",
    "    D = pd.DataFrame(y_pred[:, 1],columns = ['y_prob_inc'])\n",
    "    E = pd.DataFrame(y_true.values,columns=['Actual_Inc'])\n",
    "    Results = pd.concat([D,E],1)\n",
    "    del D,E\n",
    "    bcut = [1,.9,0]\n",
    "    label = ['top 10%','rest'][::-1]\n",
    "    bins = np.unique(algos.quantile(Results['y_prob_inc'],bcut))\n",
    "    if len(label) != len(bins) - 1:\n",
    "        return 0\n",
    "    else:\n",
    "        Results['decile_inc']  = pd.tools.tile._bins_to_cuts(Results['y_prob_inc'], bins,labels=label, include_lowest=True)\n",
    "\n",
    "        #Results['decile_inc'] = pd.qcut(Results.y_prob_inc,10,labels=range(10))\n",
    "        top = Results[Results['decile_inc'] =='top 10%'] \n",
    "        #ults.groupby(['decile_inc'])['y_prob_inc'].describe()\n",
    "        return float(top[top['Actual_Inc'] == 1].shape[0])/float(Results[Results['Actual_Inc'] == 1].shape[0])\n",
    "    \n",
    "def my_custom_loss_func2(y_true,y_pred):\n",
    "    \n",
    "    D = pd.DataFrame(y_pred[:, 1],columns = ['y_prob_inc'])\n",
    "    E = pd.DataFrame(y_true.values,columns=['Actual_Inc'])\n",
    "    Results = pd.concat([D,E],1)\n",
    "    del D,E\n",
    "    bcut = [1,.8,0]\n",
    "    label = ['top 20%','rest'][::-1]\n",
    "    bins = np.unique(algos.quantile(Results['y_prob_inc'],bcut))\n",
    "    if len(label) != len(bins) - 1:\n",
    "        return 0\n",
    "    else:\n",
    "        Results['decile_inc']  = pd.tools.tile._bins_to_cuts(Results['y_prob_inc'], bins,labels=label, include_lowest=True)\n",
    "\n",
    "        #Results['decile_inc'] = pd.qcut(Results.y_prob_inc,10,labels=range(10))\n",
    "        top = Results[Results['decile_inc'] =='top 20%'] \n",
    "        #ults.groupby(['decile_inc'])['y_prob_inc'].describe()\n",
    "        return float(top[top['Actual_Inc'] == 1].shape[0])/float(Results[Results['Actual_Inc'] == 1].shape[0])\n",
    "\n",
    "def cust_scorer():\n",
    "    return make_scorer(my_custom_loss_func, greater_is_better=True,needs_proba=True)\n",
    "from sklearn.metrics import make_scorer\n",
    "def my_custom_loss_func(y_true,y_pred):\n",
    "    \n",
    "    D = pd.DataFrame(y_pred[:, 1],columns = ['y_prob_inc'])\n",
    "    E = pd.DataFrame(y_true.values,columns=['Actual_Inc'])\n",
    "    Results = pd.concat([D,E],1)\n",
    "    del D,E\n",
    "    bcut = [1,.9,0]\n",
    "    label = ['top 10%','rest'][::-1]\n",
    "    bins = np.unique(algos.quantile(Results['y_prob_inc'],bcut))\n",
    "    if len(label) != len(bins) - 1:\n",
    "        return 0\n",
    "    else:\n",
    "        Results['decile_inc']  = pd.tools.tile._bins_to_cuts(Results['y_prob_inc'], bins,labels=label, include_lowest=True)\n",
    "\n",
    "        #Results['decile_inc'] = pd.qcut(Results.y_prob_inc,10,labels=range(10))\n",
    "        top = Results[Results['decile_inc'] =='top 10%'] \n",
    "        #ults.groupby(['decile_inc'])['y_prob_inc'].describe()\n",
    "        return float(top[top['Actual_Inc'] == 1].shape[0])/float(Results[Results['Actual_Inc'] == 1].shape[0])\n",
    "    \n",
    "def my_custom_loss_func2(y_true,y_pred):\n",
    "    \n",
    "    D = pd.DataFrame(y_pred[:, 1],columns = ['y_prob_inc'])\n",
    "    E = pd.DataFrame(y_true.values,columns=['Actual_Inc'])\n",
    "    Results = pd.concat([D,E],1)\n",
    "    del D,E\n",
    "    bcut = [1,.8,0]\n",
    "    label = ['top 20%','rest'][::-1]\n",
    "    bins = np.unique(algos.quantile(Results['y_prob_inc'],bcut))\n",
    "    if len(label) != len(bins) - 1:\n",
    "        return 0\n",
    "    else:\n",
    "        Results['decile_inc']  = pd.tools.tile._bins_to_cuts(Results['y_prob_inc'], bins,labels=label, include_lowest=True)\n",
    "\n",
    "        #Results['decile_inc'] = pd.qcut(Results.y_prob_inc,10,labels=range(10))\n",
    "        top = Results[Results['decile_inc'] =='top 20%'] \n",
    "        #ults.groupby(['decile_inc'])['y_prob_inc'].describe()\n",
    "        return float(top[top['Actual_Inc'] == 1].shape[0])/float(Results[Results['Actual_Inc'] == 1].shape[0])\n",
    "\n",
    "def cust_scorer():\n",
    "    return make_scorer(my_custom_loss_func, greater_is_better=True,needs_proba=True)\n",
    "\n",
    "def assessment2(Results,num_bins):\n",
    "        bins = np.unique(algos.quantile(Results['y_prob_inc'], np.linspace(0, 1, num_bins)))\n",
    "        Results['decile']  = pd.tools.tile._bins_to_cuts(Results['y_prob_inc'], bins,labels=np.arange( 0,len(bins)-1,1)[::-1], include_lowest=True)\n",
    "        group = Results.groupby(['decile','Actual_Inc'])\n",
    "        Results2 = group.Actual_Inc.count().unstack().fillna(0).sort_index(ascending=False)\n",
    "        Results2.columns = ['Negative','Positive']\n",
    "        Total_neg,Total_pos = Results2.sum()[0],Results2.sum()[1]\n",
    "        Total_neg,Total_pos = Results2.Negative.sum(),Results2.Positive.sum()\n",
    "        Results2['Neg_perc'] = Results2['Negative']/Total_neg\n",
    "        Results2['Pos_perc'] = Results2['Positive']/Total_pos\n",
    "        Results2['Total_perc'] = (Results2['Negative']+Results2['Positive'])/(Total_neg+Total_pos)\n",
    "        Results2['Cul_neg_perc'] = Results2['Neg_perc'].cumsum(axis=0)\n",
    "        Results2['Cul_pos_perc'] = Results2['Pos_perc'].cumsum(axis=0)\n",
    "        Results2['Cul_tot_perc'] = Results2['Total_perc'].cumsum(axis=0)\n",
    "        Results2['Lift_perc'] = (Results2['Cul_pos_perc']+0.0001)/(Results2['Cul_neg_perc']+0.0001)\n",
    "        Results2['ratio'] =  (Results2['Pos_perc']+0.0001)/(Results2['Neg_perc']+0.0001)\n",
    "        # Gain chart: Cumulative %pos and Cummulative %Population\n",
    "        # Lift chart: Decile & model\n",
    "        KS_value = max(Results2['Cul_pos_perc']-Results2['Cul_neg_perc'])\n",
    "        #print 'KS value :{0}'.format(KS_value) \n",
    "        #Results2['KS_value'] = KS_value\n",
    "        num_dec = sum(x>=y for x, y in zip(Results2['ratio'], Results2['ratio'][1:]))\n",
    "        #print \"{0} out of {1} is strictly decreasing\".format(num_dec,len(Results2['ratio'])-1) \n",
    "        mono = str(num_dec)+' out of '+str(len(Results2['ratio'])-1)\n",
    "        #Results2['mono'] =mono\n",
    "        #print \"KS_value: \",KS_value\n",
    "        #print \"monotonicity: \",mono\n",
    "       \n",
    " \n",
    "        \n",
    "        return KS_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Read cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('/data2/GMC/sample_dev_cln2.csv')\n",
    "df_h2 = pd.read_csv('/data2/GMC/sample_allattrib_oot_cln2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "droplist2 = [i for i in df2.columns.values if i[4:15]=='PDUE_BUCKET']\n",
    "df2 = df2.drop(droplist2,1)\n",
    "df2 = df2.drop(droplist2,1)\n",
    "y= df2['NEW_BAD']\n",
    "X = df2.drop(['NEW_BAD','SEGMENT_PM2016'],1)\n",
    "Xh = df_h2.drop(['NEW_BAD','SEGMENT_PM2016'],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keeplist1 = pd.read_table('keeplist',header=None)[0].values\n",
    "#df3 = df2[keeplist1]\n",
    "y= df2['NEW_BAD']\n",
    "X = df2[keeplist1]\n",
    "Xh = df_h2[keeplist1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X & y and split to train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size=0.3, random_state=3523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199115, 1385)\n",
      "(615098, 1385)\n"
     ]
    }
   ],
   "source": [
    "print Xh.shape\n",
    "print X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.977135\n",
      "1    0.022865\n",
      "Name: NEW_BAD, dtype: float64\n",
      "0    1\n",
      "Name: NEW_BAD, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print df2.NEW_BAD.value_counts(normalize=True)\n",
    "print df_h2.NEW_BAD.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(Xh.columns) - set(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run different ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214.895590067\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "forest = RandomForestClassifier(n_estimators=500, \n",
    "                                    #class_weight='balanced_subsample',\n",
    "                                    max_depth = 80,\n",
    "                                    min_samples_leaf=500,\n",
    "                                    #max_features=100,\n",
    "                                    n_jobs=-1)\n",
    "\n",
    "model = forest\n",
    "\n",
    "import time\n",
    "start = time.time()    \n",
    "model.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.61329245441\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "bad_rate = float(len(y[y==1]))/len(y)\n",
    "min_child_weight=1/math.sqrt(bad_rate)\n",
    "print min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545.667114973\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "xgb1=XGBClassifier(learning_rate=0.1, n_estimators=100,max_depth = 8,subsample=0.8, colsample_bytree =0.5)\n",
    "model = xgb1\n",
    "import time\n",
    "start = time.time()    \n",
    "model.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860.400943041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc1=GradientBoostingClassifier(learning_rate=0.05, n_estimators=200,max_depth = 3,subsample=0.8)\n",
    "model = gbc1\n",
    "import time\n",
    "start = time.time()    \n",
    "model.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3854.47664595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc2=GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth = 10,subsample=1)\n",
    "model = gbc2\n",
    "import time\n",
    "start = time.time()    \n",
    "model.fit(X_train,y_train)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Performance:  KS, ROC, top10 and 20 capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         KS_value       ROC  Top_10%_capture  Top_20%_capture\n",
      "Train    0.682072  0.921978         0.684269         0.866504\n",
      "Holdout  0.655704  0.909994         0.656988         0.840652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "proba= model.predict_proba(X_train)\n",
    "\n",
    "D = pd.DataFrame(proba.T[1],columns = ['y_prob_inc'])\n",
    "E = pd.DataFrame(y_train.values,columns=['Actual_Inc'])\n",
    "Results = pd.concat([D,E],1)\n",
    "r1 = assessment2(Results,11)\n",
    "r2 = roc_auc_score(y_train,proba[:,1])\n",
    "r3 =  my_custom_loss_func(y_train,proba)\n",
    "r4 = my_custom_loss_func2(y_train,proba)\n",
    "Result_train = (r1,r2,r3,r4)\n",
    "\n",
    "proba= model.predict_proba(X_test)\n",
    "D = pd.DataFrame(proba.T[1],columns = ['y_prob_inc'])\n",
    "E = pd.DataFrame(y_test.values,columns=['Actual_Inc'])\n",
    "Results = pd.concat([D,E],1)\n",
    "r1 = assessment2(Results,11)\n",
    "r2 = roc_auc_score(y_test,proba[:,1])\n",
    "r3 = my_custom_loss_func(y_test,proba)\n",
    "r4 = my_custom_loss_func2(y_test,proba)\n",
    "Result_test = (r1,r2,r3,r4)\n",
    "\"\"\"\n",
    "proba= model.predict_proba(Xh)\n",
    "D = pd.DataFrame(proba.T[1],columns = ['y_prob_inc'])\n",
    "E = pd.DataFrame(yh.values,columns=['Actual_Inc'])\n",
    "Results = pd.concat([D,E],1)\n",
    "r1 = assessment2(Results,11)\n",
    "r2 = roc_auc_score(yh,proba[:,1])\n",
    "r3 = my_custom_loss_func(yh,proba)\n",
    "r4 = my_custom_loss_func2(yh,proba)\n",
    "Result_valid1 = (r1,r2,r3,r4)\"\"\"\n",
    "\n",
    "Result_final = pd.DataFrame()\n",
    "Result_final['Train'] = pd.Series(Result_train)\n",
    "Result_final['Holdout'] = pd.Series(Result_test)\n",
    "#Result_final['Validation1'] = pd.Series(Result_valid1)\n",
    "Result_final = Result_final.T\n",
    "Result_final.columns =['KS_value', 'ROC', 'Top_10%_capture','Top_20%_capture']\n",
    "print Result_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the score code for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "d = joblib.dump(model, '/data2/GMC/sales_model/pickle/region_xgb1.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "d = joblib.dump(model, '/data2/GMC/sales_model/pickle/region_xgb2.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "d = joblib.dump(model, '/data2/GMC/sales_model/pickle/region_xgb3.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "d = joblib.dump(model, '/data2/GMC/sales_model/pickle/region_rf1.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "d = joblib.dump(model, '/data2/GMC/sales_model/pickle/region_rf2.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the score code, and score validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/data2/GMC/sales_model/pickle/region_xgb1.pkl') \n",
    "score = model.predict_proba(Xh)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/data2/GMC/sales_model/pickle/region_xgb2.pkl') \n",
    "score2 = model.predict_proba(Xh)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/data2/GMC/sales_model/pickle/region_xgb3.pkl') \n",
    "score3 = model.predict_proba(Xh)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/data2/GMC/sales_model/pickle/region_rf1.pkl') \n",
    "score4 = model.predict_proba(Xh)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load('/data2/GMC/sales_model/pickle/region_rf2.pkl') \n",
    "score5 = model.predict_proba(Xh)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = pd.concat([pd.Series(score),pd.Series(score2),pd.Series(score3),pd.Series(score4),pd.Series(score5)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output.index = Xh.Seqnum.values\n",
    "output.columns = ['score1','score2','score3','score4','score5']\n",
    "output.to_csv('sample_ml_result1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cols = X_train.columns\n",
    "def var_importance(model):\n",
    "\n",
    "    feature_importance = model.feature_importances_\n",
    "    # make importances relative to max importance\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "    top_sorted_idx = sorted_idx[:20]\n",
    "    pos = np.arange(top_sorted_idx.shape[0]) + .5\n",
    "    #fig = plt.figure() \n",
    "    plt.barh(pos, feature_importance[top_sorted_idx][::-1], align='center')\n",
    "    plt.yticks(pos, cols[top_sorted_idx][::-1])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    #plt.savefig(\"var_importance1.png\")\n",
    "    plt.show()\n",
    "var_importance(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
